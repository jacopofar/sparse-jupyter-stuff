{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snippets to import mood tracking data from Dailyo CSV export in a Pandas dataframe and get the words associated to the best and worst moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:15.583120Z",
     "start_time": "2018-02-11T20:38:14.835224Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('sentiment/daylio_export.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:18.777654Z",
     "start_time": "2018-02-11T20:38:18.675405Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Daylio uses this awful format: a column with the year,\n",
    "# a column with the month name in English and the day number (e.g. June 10), and another with the time\n",
    "# So this ugly procedure is necessary\n",
    "\n",
    "def to_date(r):\n",
    "    datetime_object = datetime.strptime(r['date'].split()[0] + ' ' + r['date'].split()[1].zfill(2) + ' ' + str(r['year']) + ' ' + r['time'],\n",
    "                                        '%B %d %Y %H:%M')\n",
    "    return datetime_object\n",
    "df['new_date'] = df.apply(to_date, axis=1)\n",
    "df = df.set_index('new_date')\n",
    "df = df.drop(['year', 'date','weekday', 'time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:20.859993Z",
     "start_time": "2018-02-11T20:38:20.832011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a number for the mood value\n",
    "df['mood'] = df.apply(lambda x: ['awful','fugly','meh','good','rad'].index(x['mood']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:22.734926Z",
     "start_time": "2018-02-11T20:38:22.179612Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "df['mood'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:29.057303Z",
     "start_time": "2018-02-11T20:38:29.049076Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['weekday'] = df.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:38:34.437867Z",
     "start_time": "2018-02-11T20:38:34.308453Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_dict = df.to_dict(orient='index')\n",
    "# a dictionary with this key-value structure: \n",
    "# Timestamp('2016-07-07 22:03:00'): {'activities': 'work | friends',\n",
    "#  'mood': 4,\n",
    "#  'note': \"something something\"}, ...\n",
    "from collections import Counter\n",
    "import string\n",
    "# dictionary mapping each punctuation codepoint to the space character\n",
    "punctuations_dict = dict.fromkeys(map(ord, string.punctuation), ord(' '))\n",
    "words_general = Counter()\n",
    "words_moods = [Counter() for i in range(5)]\n",
    "\n",
    "for k,v in raw_dict.items():\n",
    "    if type(v['note']) != str:\n",
    "        print(f'ignored element for {k} {v}, had type {type(v[\"note\"])}')\n",
    "        continue\n",
    "    # remove punctuations based on Unicode definition\n",
    "    \n",
    "    tokens =  v['note'].lower().translate(punctuations_dict).split()\n",
    "    words_general.update(tokens)\n",
    "    words_moods[v['mood']].update(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:39:02.857487Z",
     "start_time": "2018-02-11T20:39:02.851772Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(words_general))\n",
    "words_general.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T09:22:00.031716Z",
     "start_time": "2017-07-11T09:21:59.986738Z"
    }
   },
   "source": [
    "Now we have the word frequencies for each mood and in general.\n",
    "\n",
    "For each word we can calculate the ratio between the occurrences in some mood and the total occurrences, to get a measure of the correlation between the word and the mood.\n",
    "\n",
    "Words appearing a few times or just once, however, would have an excessively high polarity (that is, the correlation with a specific category); we need to not give too much weight to them. This is called smoothing and is a common step in Bayes models. There are various methods, based in general on the idea of adding an \"hidden\" number of occurrences for each token/category combination, an approach stemming from the assumption that by sampling we ignored some values that would have appeared with a bigger (or infinite) sample size.\n",
    "\n",
    "I'll use the simplest smoothing, called Laplacian smoothing: just add a value for each token/category combination. This way, the polarity of tokens appearing few times will be spread between all the categories, while more common tokens will have a more defined polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:39:12.912254Z",
     "start_time": "2018-02-11T20:39:12.887151Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for each token, add a fake sample to each mood pretending it appeared at least once for each\n",
    "# if a mood actually had that token, it will just appear one more time\n",
    "for w in words_general:\n",
    "    for counter in words_moods:\n",
    "        counter.update([w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the polarity of the tokens for each category is calculated as the number of occurrences for that category divided by the total number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-11T20:39:14.629859Z",
     "start_time": "2018-02-11T20:39:14.620810Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polarities = {}\n",
    "for w in words_general:\n",
    "    # words_general was not smoothed, so it still contains the actual count\n",
    "    total = words_general[w] + len(words_moods)\n",
    "    polarities[w] = [counter[w]/total for counter in words_moods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-11T10:20:26.783173Z",
     "start_time": "2017-07-11T10:20:26.769359Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "for m in range(5):\n",
    "    print(f'tokens with highest polarity with class {m}:')\n",
    "    higher = heapq.nlargest(10,polarities.items(), key=lambda x:x[1][m])\n",
    "    print(list(map(lambda k: k[0], higher)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
